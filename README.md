# TML Assignment 2

## Model Stealing

For this assignment, we were tasked with stealing a model of unkown architecture hidden behind an API,
which was also protected using B4B.


## Data Accessible to the Adversary  
- Trained encoder of unknown architecture hidden behind an API and protected using B4B  
- Subset of encoder's training data
 

## Approach Used  

We tried to use Data Augmentations & Siamese Framework as mentioned in the lecture and combined it with active leaning to minimize the number of queries to the api and hence 
the impact of B4B. In the first round we sample 1000 random images from the dataset and send them to the api. We train then our encoder to minimize the distance between its 
embeddings and the output of the api. The trained model is then used to estimate the model uncertanty for each point using MC-Dropout. we then sample the next 1000 datapoints
according to both uncertanty and the original class distribution to not activate B4B.

## Other Ideas and Implementation Details  
For 13000 images we took original image representation twice considering there might be some changes because of B4B defense. Also three representations of the same image using three types of augmentation such as horizontal flip, 15degree rotation,color jitter. As a result for each image we had 5 representations and then we trained our encoder in the following way:

X -> Encoder(X) ->2048->Projector-> output of 1024 dimension

Loss = mse(output, rep1)+cosine(output, rep1) +
mse(output, rep2)+cosine(output, rep2) +
mse(output, rep3)+cosine(output, rep3) +
mse(output, rep4)+cosine(output, rep4) +
mse(output, rep5)+cosine(output, rep5)

As encoder we used Resnet50. At the end of training, we compute L2 distances of some images between representations generated by this encoder and the API. The distance is approximately 0.001. However, after submitting the model to the server, the generated L2 distance is catastrophic approximately 36.


### Which basemodel?  
We tried different model types and found that Resnet18 resulted in the smallest average loss. 
Resnet50 for example had a L2 distance of ~36.

### How was the model trained ?
We train one model on the entire dataset.
The model is trained on the new 1000 emebddings and every 5 rounds on all data to prevent forget.

### Which loss functions were used?
We combined two different loss functions.
At start of each training we only consider MSE but then slowly introduce cosine embeddings loss as well.
Using a constrastive loss functions as well lead to a worse perfomance and sometimes and increasing loss over the epochs.

### Which Augmentations where used?  
We experimented with multiple augmentations of different strengths, i. e., cropping, removing parts of the image, etc...
We ended up only using small augmentations like flip and rotate as stronger ones activated B4B too quickly and resulted in a larger L2 distance.

### Why only train the models for X epochs?  
We observed that the model perfomance hardly changed after ~15-20 epochs during trainin so we stopped here to prevent overfitting.
When training on the full dataset we use 50 epochs.

## Results  
The above approach results in a L2 distance of 4.81. 
Our solution ranked 4th on the leaderboard.

## Observations  
The cosine similarity of two unrelated sets on a fresh encoder was 0.975 which seems very high.


## Files and Their Descriptions  

attack.py - Main code for the model stealing
            attack: runs the attack for n rounds, each round sending 1000 new images to the api. Returns the stolen model at the end


api.py - Handles all the code which interacts with the api 
        new_api: Requests a new api/model. Returns TOKEN and SEED. 
        model_stealing: Sends 1000 images to the api. Returns the embeddings. 
        upload: Uploads the current stolen model for evaluation. Returns L2 distance to orignial model. 
        embeddings_sim: Meassures the cosine similarity between 2 different subsets. Returns the average cosine similarity. 


utils.py - This file includes different functions required to steal the model. 
           augment_image and augment_image_single: Augment batch of images and a single image according to AUGMENT and AUGMENT_SINGLE. Return the augmented version.
           bayesian_predictions: Uses Mc-Droput to compute the uncertatny for each Datapoint. Returns the mean std over the embedding. 
           predict_next: Uses bayesian_predictions and the classdistirbution to generate new candidates. Returns the next 1000 images to be send to the api. 
           train: trains the models output to be close to the real encoders outputs. Returns the trained model. 


model.py - The predicted model architecture is stored here. 


data_sets.py - In here, the  Dataset classes are stored, one for the original data and one for the stolen embeddings together with the predicted embeddings. 



## Dependencies

The following Python packages and modules are required to run the scripts:


- torch
- torchvision
- requests
- onnxruntime
- tqdm
- kornia
- timm
- scikit-learn


### Custom modules
- src.utils 
- src.dataset 
- src.model 
- src.api 
